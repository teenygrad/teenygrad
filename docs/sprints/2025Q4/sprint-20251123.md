# Week ending - 23/Nov/2025

## Sprint Goal

- Get a sub 50us kernel on nvidia competition.

### Sprint Tasks

1. Submit a sub 50us entry to GPU mode.

### MVP Tasks

1. Post 1 video on youtube and promote.
2. Investigate CuteDSL and NVidia blackwell arch.
3. Convert llvm.func to tt.func.
4. Convert tl::arange op to tt.make_range.
5. Convert tl::load op to tt.load.
6. Convert tl::store op to tt.store.
7. Convert "convert i32 addition" to triton (requires promoting i32 to i64).
8. Convert "convert i32 multiplication" to triton (requires promoting i32 to i64).
9. Convert "less than operation" (uses arith dialect but with a tensor parameter).
10. Full tensor addition kernel compiled to PTX.
11. Add basic infra for running the kernel.
12. Run and test.

### Tech debt

- egg lang schema is not fit for purpose due to being too generic, this needs to be changed.
- conversions from pytorch schema to our internal schema is not the way Rust does things, this needs to be revisited.
- better error types (look at all error modules to ensure they are fit for purpose)
- centralise rustx compiler dependencies at the workspace level
- investigate the rust lints undefined cfgs in projects such as rustc_llvm
- llvm and triton should be pre-built and available via CI to improve compiler times
- improve compilation speed now that the project is much bigger
- handle constexpr using generics

### Sprint Review

- Was sprint goal achieved?

No, I got sucked into a deep dive into cutlass and cute-dsl. Still haven't formed a strong intuitive grasp of cute layouts and their algebra. Not having ready access to a b200 didn't help, although I do now have a kernel that compiles for sm100 which I intend to test.

- Good
  - Much better understanding of cutlass, cute and the Hopper and Blackwell Tensor core programming.

- Bad
  - Not sure this detour was worth it.

- Ugly
  - Cute dsl is very tempremental in it's supported python for JIT and kernels.

---------------------

import torch

import triton
import triton.language as tl

DEVICE = triton.runtime.driver.active.get_active_torch_device()

@triton.jit
def add_kernel(x_ptr,  # *Pointer* to first input vector.
               y_ptr,  # *Pointer* to second input vector.
               output_ptr,  # *Pointer* to output vector.
               n_elements,  # Size of the vector.
               BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.
               # NOTE: `constexpr` so it can be used as a shape value.
               ):
    # There are multiple 'programs' processing different data. We identify which program
    # we are here:
    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.
    # This program will process inputs that are offset from the initial data.
    # For instance, if you had a vector of length 256 and block_size of 64, the programs
    # would each access the elements [0:64, 64:128, 128:192, 192:256].
    # Note that offsets is a list of pointers:
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    # Create a mask to guard memory operations against out-of-bounds accesses.
    mask = offsets < n_elements
    # Load x and y from DRAM, masking out any extra elements in case the input is not a
    # multiple of the block size.
    x = tl.load(x_ptr + offsets, mask=mask)
    y = tl.load(y_ptr + offsets, mask=mask)
    output = x + y
    # Write x + y back to DRAM.
    tl.store(output_ptr + offsets, output, mask=mask)

def add(x: torch.Tensor, y: torch.Tensor):
    # We need to preallocate the output.
    output = torch.empty_like(x)
    assert x.device == DEVICE and y.device == DEVICE and output.device == DEVICE
    n_elements = output.numel()
    # The SPMD launch grid denotes the number of kernel instances that run in parallel.
    # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].
    # In this case, we use a 1D grid where the size is the number of blocks:
    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )
    # NOTE:
    #  - Each torch.tensor object is implicitly converted into a pointer to its first element.
    #  - `triton.jit`'ed functions can be indexed with a launch grid to obtain a callable GPU kernel.
    #  - Don't forget to pass meta-parameters as keywords arguments.
    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)
    # We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called, the kernel is still
    # running asynchronously at this point.
    return output

torch.manual_seed(0)
size = 98432
x = torch.rand(size, device=DEVICE)
y = torch.rand(size, device=DEVICE)
output_torch = x + y
output_triton = add(x, y)
print(output_torch)
print(output_triton)
print(f'The maximum difference between torch and triton is '
      f'{torch.max(torch.abs(output_torch - output_triton))}')

--
module {
  tt.func public @add_kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("/home/arshadm/Workspace/projects/spinorml/teenygrad/kernel.py":10:0), %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("/home/arshadm/Workspace/projects/spinorml/teenygrad/kernel.py":10:0), %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("/home/arshadm/Workspace/projects/spinorml/teenygrad/kernel.py":10:0), %arg3: i32 {tt.divisibility = 16 : i32} loc("/home/arshadm/Workspace/projects/spinorml/teenygrad/kernel.py":10:0)) attributes {noinline = false} {
    %0 = tt.get_program_id x : i32 loc(#loc1)
    %c1024_i32 = arith.constant 1024 : i32 loc(#loc2)
    %c1024_i32_0 = arith.constant 1024 : i32 loc(#loc2)
    %1 = arith.extsi %0 : i32 to i64 loc(#loc2)
    %2 = arith.extsi %c1024_i32_0 : i32 to i64 loc(#loc2)
    %3 = arith.muli %1, %2 : i64 loc(#loc2)
    %c2147483647_i64 = arith.constant 2147483647 : i64 loc(#loc2)
    %c-2147483648_i64 = arith.constant -2147483648 : i64 loc(#loc2)
    %4 = arith.cmpi sle, %3, %c2147483647_i64 : i64 loc(#loc2)
    %5 = arith.cmpi sge, %3, %c-2147483648_i64 : i64 loc(#loc2)
    %6 = arith.andi %4, %5 : i1 loc(#loc2)
    %7 = arith.muli %0, %c1024_i32_0 : i32 loc(#loc2)
    %8 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32> loc(#loc3)
    %9 = tt.splat %7 : i32 -> tensor<1024xi32> loc(#loc4)
    %10 = arith.extsi %9 : tensor<1024xi32> to tensor<1024xi64> loc(#loc4)
    %11 = arith.extsi %8 : tensor<1024xi32> to tensor<1024xi64> loc(#loc4)
    %12 = arith.addi %10, %11 : tensor<1024xi64> loc(#loc4)
    %c2147483647_i64_1 = arith.constant 2147483647 : i64 loc(#loc4)
    %c-2147483648_i64_2 = arith.constant -2147483648 : i64 loc(#loc4)
    %cst = arith.constant dense<2147483647> : tensor<1024xi64> loc(#loc4)
    %13 = arith.cmpi sle, %12, %cst : tensor<1024xi64> loc(#loc4)
    %cst_3 = arith.constant dense<-2147483648> : tensor<1024xi64> loc(#loc4)
    %14 = arith.cmpi sge, %12, %cst_3 : tensor<1024xi64> loc(#loc4)
    %15 = arith.andi %13, %14 : tensor<1024xi1> loc(#loc4)
    %16 = arith.addi %9, %8 : tensor<1024xi32> loc(#loc4)
    %17 = tt.splat %arg3 : i32 -> tensor<1024xi32> loc(#loc5)
    %18 = arith.cmpi slt, %16, %17 : tensor<1024xi32> loc(#loc5)
    %19 = tt.splat %arg0 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>> loc(#loc6)
    %20 = tt.addptr %19, %16 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32> loc(#loc6)
    %21 = tt.load %20, %18 : tensor<1024x!tt.ptr<f32>> loc(#loc7)
    %22 = tt.splat %arg1 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>> loc(#loc8)
    %23 = tt.addptr %22, %16 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32> loc(#loc8)
    %24 = tt.load %23, %18 : tensor<1024x!tt.ptr<f32>> loc(#loc9)
    %25 = arith.addf %21, %24 : tensor<1024xf32> loc(#loc10)
    %26 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>> loc(#loc11)
    %27 = tt.addptr %26, %16 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32> loc(#loc11)
    tt.store %27, %25, %18 : tensor<1024x!tt.ptr<f32>> loc(#loc12)
    tt.return loc(#loc13)
  } loc(#loc)
} loc(#loc)
