/*
 * Copyright \(c\) 2025 Teenygrad. All rights reserved.
 *
 * This program is free software: you can redistribute it and/or modify it
 * under the terms of the GNU General Public License as published by the
 * Free Software Foundation, either version 3 of the License, or (at your
 * option) any later version.
 *
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 * General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program. If not, see <https://www.gnu.org/licenses/>.
 */

//  # Copyright (c) Meta Platforms, Inc. and affiliates.
// # This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.

// import math
// from dataclasses import dataclass
// from typing import Optional

// import fairscale.nn.model_parallel.initialize as fs_init
// import torch
// import torch.nn.functional as F
// from fairscale.nn.model_parallel.layers import (
//     ColumnParallelLinear,
//     RowParallelLinear,
//     VocabParallelEmbedding,
// )
// from torch import nn
// from .math_ops import MathOps
// from benchmarking import Profiler

// @dataclass
// class ModelArgs:
//     dim: int = 4096
//     n_layers: int = 32
//     n_heads: int = 32
//     n_kv_heads: Optional[int] = None
//     vocab_size: int = -1
//     multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2
//     ffn_dim_multiplier: Optional[float] = None
//     norm_eps: float = 1e-5
//     rope_theta: float = 500000

//     max_batch_size: int = 32
//     max_seq_len: int = 2048

// def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:
//     """torch.repeat_interleave(x, dim=2, repeats=n_rep)"""
//     bs, slen, n_kv_heads, head_dim = x.shape
//     if n_rep == 1:
//         return x
//     return (
//         x[:, :, :, None, :]
//         .expand(bs, slen, n_kv_heads, n_rep, head_dim)
//         .reshape(bs, slen, n_kv_heads * n_rep, head_dim)
//     )

// class Attention(nn.Module):
//     def __init__(self, args: ModelArgs, use_triton=False):
//         super().__init__()
//         self.use_triton = use_triton
//         self.Math = MathOps(use_triton)
//         self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads
//         model_parallel_size = fs_init.get_model_parallel_world_size()
//         self.n_local_heads = args.n_heads // model_parallel_size
//         self.n_local_kv_heads = self.n_kv_heads // model_parallel_size
//         self.n_rep = self.n_local_heads // self.n_local_kv_heads
//         self.head_dim = args.dim // args.n_heads

//         self.wq = ColumnParallelLinear(
//             args.dim,
//             args.n_heads * self.head_dim,
//             bias=False,
//             gather_output=False,
//             init_method=lambda x: x,
//         )
//         self.wk = ColumnParallelLinear(
//             args.dim,
//             self.n_kv_heads * self.head_dim,
//             bias=False,
//             gather_output=False,
//             init_method=lambda x: x,
//         )
//         self.wv = ColumnParallelLinear(
//             args.dim,
//             self.n_kv_heads * self.head_dim,
//             bias=False,
//             gather_output=False,
//             init_method=lambda x: x,
//         )
//         self.wo = RowParallelLinear(
//             args.n_heads * self.head_dim,
//             args.dim,
//             bias=False,
//             input_is_parallel=True,
//             init_method=lambda x: x,
//         )

//         self.cache_k = torch.zeros(
//             (
//                 args.max_batch_size,
//                 args.max_seq_len,
//                 self.n_local_kv_heads,
//                 self.head_dim,
//             )
//         ).cuda()
//         self.cache_v = torch.zeros(
//             (
//                 args.max_batch_size,
//                 args.max_seq_len,
//                 self.n_local_kv_heads,
//                 self.head_dim,
//             )
//         ).cuda()

//     @Profiler.profiling_decorator(record_name="attention_forward", skip_profiling=True)
//     def forward(
//         self,
//         x: torch.Tensor,
//         start_pos: int,
//         freqs_cis: torch.Tensor,
//         mask: Optional[torch.Tensor],
//     ):
//         bsz, seqlen, _ = x.shape
//         xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)

//         xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)
//         xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)
//         xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)

//         xq, xk = self.Math.apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)

//         self.cache_k = self.cache_k.to(xq)
//         self.cache_v = self.cache_v.to(xq)

//         self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk
//         self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv

//         keys = self.cache_k[:bsz, : start_pos + seqlen]
//         values = self.cache_v[:bsz, : start_pos + seqlen]

//         # repeat k/v heads if n_kv_heads < n_heads
//         keys = repeat_kv(
//             keys, self.n_rep
//         )  # (bs, cache_len + seqlen, n_local_heads, head_dim)
//         values = repeat_kv(
//             values, self.n_rep
//         )  # (bs, cache_len + seqlen, n_local_heads, head_dim)

//         xq = xq.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)
//         keys = keys.transpose(1, 2)  # (bs, n_local_heads, cache_len + seqlen, head_dim)
//         values = values.transpose(
//             1, 2
//         )  # (bs, n_local_heads, cache_len + seqlen, head_dim)
//         output = self.Math.attention(xq, keys, values, self.head_dim, mask)
//         output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)
//         return self.wo(output)

// class FeedForward(nn.Module):
//     def __init__(
//         self,
//         dim: int,
//         hidden_dim: int,
//         multiple_of: int,
//         ffn_dim_multiplier: Optional[float],
//     ):
//         super().__init__()
//         hidden_dim = int(2 * hidden_dim / 3)
//         # custom dim factor multiplier
//         if ffn_dim_multiplier is not None:
//             hidden_dim = int(ffn_dim_multiplier * hidden_dim)
//         hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)

//         self.w1 = ColumnParallelLinear(
//             dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x
//         )
//         self.w2 = RowParallelLinear(
//             hidden_dim, dim, bias=False, input_is_parallel=True, init_method=lambda x: x
//         )
//         self.w3 = ColumnParallelLinear(
//             dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x
//         )

//     @Profiler.profiling_decorator(
//         record_name="feed_forward_forward", skip_profiling=True
//     )
//     def forward(self, x):
//         return self.w2(F.silu(self.w1(x)) * self.w3(x))

// class TransformerBlock(nn.Module):
//     def __init__(self, layer_id: int, args: ModelArgs, use_triton=False):
//         super().__init__()
//         self.use_triton = use_triton
//         self.Math = MathOps(use_triton)

//         self.n_heads = args.n_heads
//         self.dim = args.dim
//         self.head_dim = args.dim // args.n_heads
//         self.attention = Attention(args, use_triton=self.use_triton)
//         self.feed_forward = FeedForward(
//             dim=args.dim,
//             hidden_dim=4 * args.dim,
//             multiple_of=args.multiple_of,
//             ffn_dim_multiplier=args.ffn_dim_multiplier,
//         )
//         self.layer_id = layer_id
//         self.attention_norm = self.Math.get_rms_norm(args.dim, eps=args.norm_eps)
//         self.ffn_norm = self.Math.get_rms_norm(args.dim, eps=args.norm_eps)

//     @Profiler.profiling_decorator(
//         record_name="transform_block_forward", skip_profiling=True
//     )
//     def forward(
//         self,
//         x: torch.Tensor,
//         start_pos: int,
//         freqs_cis: torch.Tensor,
//         mask: Optional[torch.Tensor],
//     ):
//         h = x + self.attention(self.attention_norm(x), start_pos, freqs_cis, mask)
//         out = h + self.feed_forward(self.ffn_norm(h))
//         return out

// class Transformer(nn.Module):
//     def __init__(self, params: ModelArgs, use_triton=False):
//         super().__init__()
//         self.use_triton = use_triton
//         self.Math = MathOps(use_triton)
//         self.params = params
//         self.vocab_size = params.vocab_size
//         self.n_layers = params.n_layers

//         self.tok_embeddings = VocabParallelEmbedding(
//             params.vocab_size, params.dim, init_method=lambda x: x
//         )

//         self.layers = torch.nn.ModuleList()
//         for layer_id in range(params.n_layers):
//             self.layers.append(TransformerBlock(layer_id, params, self.use_triton))

//         self.norm = self.Math.get_rms_norm(params.dim, eps=params.norm_eps)
//         self.output = ColumnParallelLinear(
//             params.dim, params.vocab_size, bias=False, init_method=lambda x: x
//         )

//         self.freqs_cis = self.Math.precompute_freqs_cis(
//             params.dim // params.n_heads,
//             params.max_seq_len * 2,
//             params.rope_theta,
//         )

//     @torch.inference_mode()
//     @Profiler.profiling_decorator(
//         record_name="transformer_forward", skip_profiling=True
//     )
//     def forward(self, tokens: torch.Tensor, start_pos: int):
//         _bsz, seqlen = tokens.shape
//         h = self.tok_embeddings(tokens)
//         self.freqs_cis = self.freqs_cis.to(h.device)
//         freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]

//         mask = None
//         if seqlen > 1:
//             mask = torch.full((seqlen, seqlen), float("-inf"), device=tokens.device)

//             mask = torch.triu(mask, diagonal=1)

//             # When performing key-value caching, we compute the attention scores
//             # only for the new sequence. Thus, the matrix of scores is of size
//             # (seqlen, cache_len + seqlen), and the only masked entries are (i, j) for
//             # j > cache_len + i, since row i corresponds to token cache_len + i.
//             mask = torch.hstack(
//                 [torch.zeros((seqlen, start_pos), device=tokens.device), mask]
//             ).type_as(h)

//         for layer in self.layers:
//             h = layer(h, start_pos, freqs_cis, mask)
//         h = self.norm(h)
//         output = self.output(h).float()
//         return output
